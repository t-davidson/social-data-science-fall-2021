---
title: "Social Data Science" 
subtitle: "Introduction to Natural Language Processing II"
author: Dr. Thomas Davidson
institute: Rutgers University
date: October 13, 2021
output:
    beamer_presentation:
      theme: "Szeged"
      colortheme: "beaver"
      fonttheme: "structurebold"
      toc: false
      incremental: false
header-includes:
  - \usepackage{multicol}
  - \usepackage{caption}
  - \usepackage{hyperref}
  - \captionsetup[figure]{font=scriptsize}
  - \captionsetup[figure]{labelformat=empty}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
library("knitr")
library("formatR")

opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
opts_chunk$set(tidy = FALSE)

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```

# Plan
1. Course updates
2. TF-IDF weighting
3. Vector representations of texts
4. Document-similarity measures

# Course updates
- Homework 2 due Friday at 5pm
  - Office hours today 4:30-5:30

# Working with text
## Recap
- Introduction to Natural Language Processing
- Pre-processing texts
  - Tokenization, stemming, stop word removal
- The bag-of-words representation
  - N-grams

# Working with text
## Comparing documents
- The goal of today's lecture is to introduce methods for comparing documents
  - Re-weighting word counts to find distinctive words
  - Representing documents as vectors of word counts
  - Using geometry to compare these vectors
  

# Working with text
## Limitations of word counts
- Word counts  alone are an imperfect measure for comparing documents
  - Some words occur in most documents, providing little information about the document (recall Zipf's law)
  - We want to find words that help distinguish between documents

# Working with text 
## Term-frequency inverse document-frequency (TF-IDF)  
- Term-frequency inverse document-frequency (TF-IDF) is a way to weight word counts ("term frequency") to give higher weights to words that help distinguish between documents
  - Intuition: Adjust word counts to take into account how many documents a word appears in 

# Working with text
## Calculating term-frequency inverse document-frequency (TF-IDF)
- $N$ = number of documents in the corpus
- $tf_{t,d}$ = number of times term $t$ used in document $d$
- $df_{t}$ = number of documents containing term $t$
- $idf_{t} = log(\frac{N}{df_{t}})$ = log of fraction of all documents containing $t$
  - $\frac{N}{df_{t}}$ is larger for terms occurring in fewer documents
  - The logarithm is used to penalize very high values
  - If a word occurs in all documents $df_{t} = N$, thus $idf_{t} = log\frac{N}{N} = log(1) = 0$ .
- We then use these values to calculate $TF\-IDF_{t,d} = tf_{t,d}*idf_{t}$

# Working with text: TF-IDF
## Loading data
Loading the word frequency objects created last lecture using `tidytext`.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
library(tidyverse)
library(ggplot2)
library(stringr)
library(tidytext)
library(gutenbergr)

texts <- read_csv('marxdurkheim.csv') # Original texts
words <- read_csv('words.csv') # Word counts and totals
head(texts)
head(words)
```


# Working with text: TF-IDF
## Computing TF-IDF in `tidytext`
We can easily compute TF-IDF weights using `tidy.text` by using the word-count object we created last lecture.  Note the two document example is quite trivial. Many words have IDF scores equal to zero.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf <- words %>% bind_tf_idf(word, title, n)
head(tidy.tfidf)
```

# Working with text: TF-IDF
Take the stem "countri" for example (short for country, country's, countries). 
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(word == "countri")
```

# Working with text: TF-IDF
The term "australia" has a relatively low term frequency but a higher IDF score, since it only occurs in *Elementary Forms*.
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(word == "australia")
```

# Working with text: TF-IDF
In this case *all* words unique to one document will have the same IDF score, log(2/1).
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
head(tidy.tfidf %>% filter(round(idf,2) == 0.69))
```

# Working with text: TF-IDF
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(title == "Communist Manifesto") %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  head(10) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(color='red') +
  labs(y = NULL, x='TF-IDF weight', title="10 stems with highest TF-IDF weight in The Communist Manifesto", caption="Stopwords removed+, stemmed")
```

# Working with text: TF-IDF
```{r, echo=FALSE, fig.width=6, fig.height=4,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
tidy.tfidf %>% filter(title == "Elementary Forms") %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  head(10) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(color='blue') +
  labs(y = NULL, x='TF-IDF weight', title="10 stems with highest TF-IDF weight in The Elementary Forms of Religious Life", caption="Stopwords removed+, stemmed")
```

# Vector representations of texts
## The document-term matrix (DTM)
- A frequently used bag-of-words representation of a text corpus is the *Document-Term Matrix*:
  - Each row* is a document (a unit of text)
  - Each column is a term (word)
  - For a given DTM $X$, each cell $X_{i,j}$ indicates the number of times a term $i$ occurs in document $j$, $tf_{i,j}$.
    - This can be the raw term counts or TF-IDF weighted counts.
- Most cells are empty so it is usually stored as a sparse matrix to conserve memory.
  
\tiny \*Sometimes the rows and columns are reversed, resulting in a *Term-Document Matrix* or *TDM*

# Vector representations of texts
## Casting a `tidytext` object into a DTM
```{r, echo=TRUE,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
X <- texts %>% unnest_tokens(word, text) %>% anti_join(stop_words) %>% count(title, word) %>% cast_dtm(title, word, n)
print(X)
```
Note that this matrix is not weighted by TF-IDF, although we could apply the weights if desired.

# Vector representations of texts
## Viewing the DTM
The object created is a class unique to the `tidytext` package. We can inspect this to see what it contains.
```{r, echo=TRUE,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
class(X)
dim(X)
X$dimnames[1]
#X$dimnames[2] # prints all columns as a long list
X$dimnames[[2]][1:50] # first 50 columns
X$v[1:50] # first 50 values
```

# Vector representations of texts
## Viewing the DTM
The easiest way to see the actual DTM is to cast it to a matrix.
```{r, echo=TRUE,tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
Xm <- as.matrix(X)
```

# Vector representations of texts
## Geometric interpretation
- Each text is a vector in N-dimensional space, where N is the total number of unique words (column of the DTM)
- Each word is a vector in D-dimensional space, where D is the number of documents (rows of the DTM)

\tiny See https://web.stanford.edu/~jurafsky/slp3/6.pdf for more details on the vector-space model

# Vector representations of texts
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/doc_vectors.png')
```
This example from Jurafsky and Martin shows a Term-Document Matrix (TDM) pertaining to four key words from four Shakespeare plays. The document vectors are highlighted in red.

# Vector representations of texts
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/vector_vis.png')
```
Here vectors for each play are plotted in two-dimensional space. The y- and x-axes indicate the number of times the words "battle" and "fool" appear in each play. Note how some vectors are closer than others and how they have different lengths.

# Vector representations of texts
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/word_vectors.png')
```
We could also treat the rows of this matrix as vector representations of each word. We will discuss this further next week when we study word embeddings.

# Vector representations of texts
## Cosine similarity
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/cosine.png')
```

# Vector representations of texts
```{r, out.width="70%",out.height="70%", fig.align="center"}
  include_graphics('../images/vector_vis_angles.png')
```

# Vector representations of texts
## Calculating cosine similarity

$\vec{u}$ and $\vec{v}$ are vectors representing texts (e.g. rows from a DTM matrix). We can compute the cosine of the angle between these two vectors using the following formula:


$$ cos(\theta) = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\|\|\vec{v}\|} = \frac{\sum_{i}\vec{u_i} \vec{v_i}}{\sqrt{\sum_{i}\vec{u}_i^2} \sqrt{\sum_{i}\vec{v}_i^2}} $$

The value range from 0 (complete dissimilarity) to 1 (identical), since all values are non-negative.

# Vector representations of texts
## Calculating cosine similarity
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
u <- c(1,2,3,4)
v <- c(0,1,0,1)

sum(u*v) / (sqrt(sum(u^2)) * sqrt(sum(v^2)))

u %*% v / (sqrt(u %*% u) * sqrt(v %*% v)) # Using matrix multiplication

```

# Vector representations of texts
## Calculating cosine similarity
Let's make a function to compute this.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
cosine.sim <- function(u,v) {
  numerator <- u %*% v
  denominator <- sqrt(u %*% u) * sqrt(v %*% v)
  return (numerator/denominator)
}

cosine.sim(u,v)
```



# Vector representations of texts
## Cosine similarity between Marx and Durkheim
We can use the two columns of the DTM matrix defined above as arguments to the similarity function.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
print(cosine.sim(Xm[1,], Xm[2,]))
```

# Vector representations of texts
## Cosine similarity for a larger corpus
The similarity between Marx's *Communist Manifesto* and Durkheim's *Elementary Forms* is rather meaningless without more information. Let's consider another example with a slightly larger corpus of texts.
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
m <- gutenberg_metadata %>% filter(author == "Shakespeare, William" & language == "en")
rj <- gutenberg_download(1112)
mnd <- gutenberg_download(1113)
tn <- gutenberg_download(1123)
kl <- gutenberg_download(1128)
mb <- gutenberg_download(1129)
rj$play <- "Romeo & Juliet"
mnd$play <- "A Midsummer Night's Dream"
tn$play <- "Twelth Night"
kl$play <- "King Lear"
mb$play <- "Macbeth"

S <- bind_rows(rj, mnd, tn, kl, mb)
```

# Vector representations of texts
## Exercise: From tidytext to DTM
Convert the plays into tidytext objects, using any preprocessing steps you want and filtering out any words which occur less than 10 times in the corpus. Calculate TF-IDF scores then convert to a DTM called `S.m`.
```{r, echo=FALSE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
S.m <- S %>% unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(play, word) %>% 
  group_by(word) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>%
  filter(total >= 10) %>% 
  bind_tf_idf(word, play, n) %>% # get TF-IDF
  cast_dtm(play, word, tf) # convert to DTM (only using tf not tf_idf, but could switch)
  
print(S.m)
dim(S.m)
```

# Vector representations of texts
## Extracting TF-IDF matrix
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
S.dense <- as.matrix(S.m)

# Run line below if using tf-idf weights, as some columns will contain zeros
#S.dense <- S.dense[,colSums(S.dense) > 0]
```

# Vector representations of texts
## Normalizing columns
We can simplify the cosine similarity calculating to a single matrix multiplication if we normalize each column by its length (the denominator in the above calculation.)
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
normalize <- function(v) {
  return (v/sqrt(v %*% v))
}

# Normalizing every column in the matrix
for (i in 1:dim(S.dense)[1]) {
  S.dense[i,] <- normalize(S.dense[i,])
}
```

# Vector representations of texts
## Calculating cosine similarity using matrix multiplication
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
sims <- S.dense %*% t(S.dense)
print(sims)
```

# Vector representations of texts
## Calculating cosine similarity using matrix multiplication
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
library(reshape2)
library(viridis)
data <- melt(sims)
colnames(data) <- c("play_i", "play_j", "similarity")

ggplot(data, aes(x = play_i, y = play_j, fill = similarity)) + geom_tile() +
  scale_fill_gradient2() +
  scale_fill_viridis_c()+
  theme_minimal() + ylim(rev(levels(data$play_i))) + xlim(levels(data$play_j))
```

# Vector representations of texts
## Calculating cosine similarity using matrix multiplication
```{r, echo=TRUE, tidy=FALSE, mysize=TRUE, size='\\footnotesize'}
sims2 <- sims
diag(sims2) <- NA # Set diagonal values to NA

data <- melt(sims2)
colnames(data) <- c("play_i", "play_j", "similarity")

ggplot(data, aes(x = play_j, y = play_i, fill = similarity)) + geom_tile() +
  scale_fill_gradient2() +
  scale_fill_viridis_c() +
  theme_minimal()  + labs (x = "", y = "", title = "Cosine similarity of Shakespeare's plays") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ylim(rev(levels(data$play_i))) + xlim(levels(data$play_j))
```

# Next week
- Word embeddings